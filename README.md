# ğŸ–¥ï¸ Dell Product & Support Page Scraper

## ğŸš€ Overview
This project includes web scrapers designed to extract product details and support page data from [Dell's website](https://www.dell.com). The extracted data is saved in structured formats for further analysis and automation.

---

## ğŸ“Œ Scripts & Workflow

### **1ï¸âƒ£ Product Scraper (`all_product_scrap.py`)**
This script extracts product data from multiple Dell product categories, including laptops, desktops, monitors, and accessories.

#### **ğŸ”„ Workflow:**
1. **Fetch Product Pages**
   - Iterates through different product categories.
   - Sends requests to Dell's website using randomized User-Agent headers.
   - Saves raw HTML pages in `webpage_templates/{category}/`.

2. **Extract Product Details**
   - Parses stored HTML files with BeautifulSoup.
   - Extracts product details including:
     - ğŸ·ï¸ **Name**
     - ğŸ’° **Price**
     - ğŸ–¼ï¸ **Image URLs** (primary & secondary images)
     - ğŸ”— **Product Link**
     - â­ **Ratings**
     - ğŸ”¹ **Features** (for laptops, desktops, etc.)
     - ğŸ“ **Specifications** (for monitors, audio, and accessories)
   - Stores extracted data in `data/details.json`.

---

### **2ï¸âƒ£ Dell Support Page Scraper (`home_scrap.py`)**
This script extracts images, hyperlinks, and buttons from the Dell Support page.

#### **ğŸ”„ Workflow:**
1. **Fetch the Support Page**
   - Sends a request to the Dell Support page.
   - Saves HTML content in `webpage_templates/pages/home.html`.

2. **Extract Relevant Data**
   - Parses the HTML to extract:
     - ğŸ–¼ï¸ **Images** (`src` and `alt` attributes)
     - ğŸ”— **Hyperlinks** (`a` tags including text and URLs)
     - ğŸ›ï¸ **Buttons** (`<button>` tags and elements styled as buttons, including clickable `div` and `a` elements)

3. **Save Extracted Data to CSV**
   - ğŸ“‚ **Images** â†’ `data/images.csv`
   - ğŸ“‚ **Links** â†’ `data/links.csv`
   - ğŸ“‚ **Buttons** â†’ `data/buttons.csv`

---

## âš™ï¸ Prerequisites
- ğŸ Python 3.x
- Install dependencies:
  - For Ubuntu/Linux: `pip install -r requirements.txt`
  - For Windows: `pip install -r win-requirements.txt`

---

## ğŸ“¥ Installation & Usage

1ï¸âƒ£ Clone the repository:
```sh
 git clone https://github.com/jatin7425/webscraping.git
 cd webscraping
```

2ï¸âƒ£ Create & activate a virtual environment (optional but recommended).

3ï¸âƒ£ Install dependencies:
```sh
 pip install -r requirements.txt  # Ubuntu/Linux
 pip install -r win-requirements.txt  # Windows
```

4ï¸âƒ£ Run the scrapers:
```sh
 python all_product_scrap.py  # Scrape product data
 python home_scrap.py  # Scrape support page data
```

---

## ğŸ“Š Sample Output

### **ğŸ“¦ Product Scraper (`data/details.json`):**
```json
[
    {
        "Product_Type": "laptop",
        "name": "Dell XPS 15",
        "price": "â‚¹1,50,000",
        "image": ["https://example.com/image1.jpg", "https://example.com/image2.jpg"],
        "link": "https://www.dell.com/xps-15",
        "ratings": "4.5 stars",
        "features": ["Intel i7", "16GB RAM", "512GB SSD"]
    }
]
```

### **ğŸ–¼ï¸ Support Page Scraper Output:**

ğŸ“‚ **images.csv:**
```
src,alt
https://example.com/image1.jpg,Support Logo
https://example.com/image2.jpg,No Alt Text
```

ğŸ“‚ **links.csv:**
```
text,url
Support Page,https://www.dell.com/support
Contact Us,https://www.dell.com/contact
```

ğŸ“‚ **buttons.csv:**
```
text
Submit
Learn More
```

